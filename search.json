[
  {
    "objectID": "netana_l06.html",
    "href": "netana_l06.html",
    "title": "Lecture 06",
    "section": "",
    "text": "We are using networkx but graphtools is faster so we try this later. Graph tool uses matrices instead of objects, so it uses 0 indexing with makes some of the things you would do vor visualizing and sparse graphs a bit stupid. But for general graph work it think it works very well. It also doesn’t have all the different types of algorithms implemented with their scientific name, but there are just a few algorithm types and the implementation while having maybe a common foundation of some algorithm is probably a modified or enhanced version that just serves this purpose well.\nThere is also a gpu version of networkx from nvdida that I would like to try later.\n\n\nThe main topic is communities and in that context as an introduction or important fundamentals connectivity and connectivity decomposition. ## Communities ### Connectivity ### Connectivity Decomposition\n\n\n\n\nThe Kronecker delta gets the symbol \\(\\delta_{ij}\\)\nIt is part of the Partitioning Chapter\nThe Kronecker delta has something to do with the laplacian matrix\nThe kronecker delta is a symbol using delta that and two indices ij to describe a duality in a matrix. It is a standard mathematical notation more than anything that is useful here, when we talk about laplacian matrices and the connection / relation of two entities.\n\n\n\n\n\nThe laplacian Matrix was also introduced here to be important, also in the Networks book.",
    "crumbs": [
      "Lecture 06"
    ]
  },
  {
    "objectID": "netana_l06.html#imports",
    "href": "netana_l06.html#imports",
    "title": "Lecture 06",
    "section": "",
    "text": "We are using networkx but graphtools is faster so we try this later. Graph tool uses matrices instead of objects, so it uses 0 indexing with makes some of the things you would do vor visualizing and sparse graphs a bit stupid. But for general graph work it think it works very well. It also doesn’t have all the different types of algorithms implemented with their scientific name, but there are just a few algorithm types and the implementation while having maybe a common foundation of some algorithm is probably a modified or enhanced version that just serves this purpose well.\nThere is also a gpu version of networkx from nvdida that I would like to try later.\n\n\nThe main topic is communities and in that context as an introduction or important fundamentals connectivity and connectivity decomposition. ## Communities ### Connectivity ### Connectivity Decomposition\n\n\n\n\nThe Kronecker delta gets the symbol \\(\\delta_{ij}\\)\nIt is part of the Partitioning Chapter\nThe Kronecker delta has something to do with the laplacian matrix\nThe kronecker delta is a symbol using delta that and two indices ij to describe a duality in a matrix. It is a standard mathematical notation more than anything that is useful here, when we talk about laplacian matrices and the connection / relation of two entities.\n\n\n\n\n\nThe laplacian Matrix was also introduced here to be important, also in the Networks book.",
    "crumbs": [
      "Lecture 06"
    ]
  },
  {
    "objectID": "netana_l06.html#partitioning",
    "href": "netana_l06.html#partitioning",
    "title": "Lecture 06",
    "section": "Partitioning",
    "text": "Partitioning\n\n⚠️ This part still needs some love. Because it is very tricky how they formulate the partitioning problem as an optimization problem that can then be formulated using the Laplacian.\n\nPartitioning is the process of separating the nodes of the graph into mutually exclusive groups \\(P_i\\). \\[V = \\dot{\\cup} P_i\\]\nSo for example a bisection is separating the graph into two equal size parts. This seems a bit primitive at first, because you would think that maybe the optimal size of each group wouldn’t be exactly half, but this is still used and important in many areas. Application areas are parallel computing and chip design where you need to minimize the transport of data / interconnection cost between processors since this communication and data exchange is slow, and dividing the system into equal-sized parts is the most controllable thing to do.\nThe simplest version of this would be to divide nodes into groups \\(g_1\\) and \\(g_2\\) which is called graph bisection. This partitioning creates a cut, so a number of edges crossing the two parts. The cut size between \\(g_1\\) and \\(g_2\\) is \\[R=\\frac{1}{2}\\sum_{i,j} A_{ij} \\text{ where } i\\in g_1, j \\in g_2\\].\nThe factor \\(\\frac{1}{2}\\) avoids double-counting undirected edges.\nThis problem can also be written in matrix terms that come with a range of benefits in speed and utility. For that we introduce a vector \\(s\\) with entries \\(s_i\\):\nFor each node \\(i\\) the value \\(s_i = \\begin{cases}\n+1: i\\in g_1\\\\\n-1: i\\in g_2\\\\\n\\end{cases}\\)\nThen \\(\\frac{1}{2}(1 - s_is_j  = \\begin{cases}\n1: i, j \\text{ are in different groups (a cut)}\\\\\n0: i, j \\text{ are in the same group}\\\\\n\\end{cases}\\) and \\(R = \\frac{1}{4} \\sum_{i,j} A_{aj} (1-s_is_j)\\)\nand then sum over all \\(i, j\\).\nWith \\(\\sum_{i,j}A_{ij}=\\sum_i k_i = \\sum_i k_i = \\sum_i k_i s_i^2 = \\sum_{i,j} k_i \\delta_{ij}s_i s_j\\) with \\(k_i\\) degree and \\(\\delta_{ij}\\) Kronecker delta (1 if equal). But then \\(R = \\frac{1}{4}\\sum_{i,j}(k_i \\delta_{ij} - A_{ij})s_i s_j\\) by substituting in above formula, thus \\(R = \\frac{1}{4}\\sum L_{ij} s_i s_j\\) where \\(L\\) is the Laplacian matrix \\(L = D - A\\), \\(D\\) diagonal degree matrix.\n\\[L = \\begin{cases}\nk_i: i = j \\\\\n-1 : i &lt;&gt;j \\text{ and } i \\text{ adjacent } j\\\\\n0 \\text{ else}\n\\end{cases}\\]\nIn matrix form \\(R = \\frac{1}{4} s^T Ls\\), where we search for a divising vector \\(s\\) on structure \\(L\\) that minimizes cut size.\nThis expression gives us a matrix formulation of the graph partitioning problem. The matrix \\(L\\) specifies the structure of our network, the vector \\(s\\) defines a division of that network into groups, and our goal is to find the vector \\(s\\) that minimizes the cut size for given \\(L\\).\nThe good thing about this Matrix formulation is that we can use spectral partitioning with eigenvectors to find a good partitioning quickly (For more see 6.14.1 in Newman). But we won’t do that here, we will look at the cost of a naive approach and then a popular algorithm to solve this partitioning problem efficiently.\nBrute force approach: Test all bisections, take the best to minimise cut. Note the number of possible bisections \\(N_{bs}\\): We choose \\(n_1\\) nodes out of \\(N\\), thus \\[N_{bs} = \\begin{pmatrix}N\\\\n_1\\end{pmatrix} = \\frac{N!}{n_1 ! (N-n_1)!} = \\frac{N!}{n_1! \\cdot n_2!}\\] for a bisection into \\(n_1, n_2\\) nodes.\nMinimizing this the size of this cut is a major problem here which is VERY hard because testing for it would need many comparisons. If we only want to bisect into equal sizes \\(\\approx \\frac{2^{n + \\frac{1}{2}}}{\\sqrt{\\pi n} }\\):\n\\(N = 10 \\rightarrow 252\\) bisections to test\n\\(N = 100 \\rightarrow 10^{29}\\) bisections to test\nBut there are algorithms that use heuristics to solve this problem efficiently. For example the Kernighan-Lin algorithm.\n\nKernighan-Lin algorithm for Partitioning of Networks\nThe Kernighan-Lin algorithm is one of the most popular algorithms for solving the two-way partitioning problem. The algorithm was described first in this paper, though the explanation here is mostly in line with the presentation in this video.\nThe algorithm uses iterative improvement, starting at an initial partition of a network into parts \\((A, B)\\) such that \\(|A|=n=|B|\\), and \\(A\\cap B = \\emptyset\\). Now \\(P = {A, B}\\) is the initial partition and \\(P^* = {A^*, B^*}\\) is the optimum partition. Now to get to this optimum partition \\(P^*\\) from \\(P\\), one has to swap a subset \\(X\\) of \\(A\\) with a subset \\(Y\\) of \\(B\\) such that,\n* \\((1) |X| = |Y|\\) * \\((2) X = A\\cap B^*\\) * \\((3) Y = A^* \\cap B\\)\nSo essentially in each step we find a subset \\(X\\) in \\(A\\) and \\(Y\\) in \\(B\\), that are the same size, so when we swap these two the cut size of the two partitions improves. Obviously there is an optimal solution that can be reached here of a minimal cut size between two partitions of the same size. Finding this is very costly, but the Kernighan-Lin algorithm gives us a way to compute this relatively efficiently.\nWe define the external cost of node \\(a\\) \\(E_a\\) as: &gt; Consider any node \\(a\\) in block \\(A\\). The contribution of node \\(a\\) to the cutset is called the external cost of a and is denoted as \\(E_a\\), where \\(E_a = \\sum_{v\\in B}c_{av}\\)\nand the internal cost \\(I_a\\) as: &gt; The internal cost \\(I_a\\) of node \\(a \\in A\\) is defined as \\(I_a = \\sum_{v\\in A}c_{av}\\)\nMoving node \\(a\\) from block \\(A\\) to block \\(B\\) would increase the value of the cutset by \\(I_a\\) and decrease it by \\(E_a\\). Therefore, the benefit/profit of moving \\(a\\) from \\(A\\) to \\(B\\) is \\(D_a = E_a - I_a\\)\nTo maintain balanced partition, we must move a node from \\(B\\) to \\(A\\) each time we move a node from \\(A\\) to \\(B\\). The effect of swapping two modules \\(a\\in A\\) with \\(b\\in B\\) is characterized as follows.\n\nIf two elements \\(a\\in A\\) and \\(b\\in B\\) are interchanged, the reduction in the cost is given by \\(g_{ab} = D_a + D_b - 2c_{ab}\\)\n\nSwapping affects nodes attached to the swapped nodes &gt; If two elements \\(a\\in A\\) and \\(b\\in B\\) are interchanged, then the new \\(D\\)-values, indicated by \\(D'\\), are given by \\[D'_x = D_x + 2c_{xa} - 2c_{xb}, \\forall x \\in A - \\{a\\}\\] \\[D'_y = D_y + 2c_{yb} - 2c_{ya}, \\forall y \\in B - \\{b\\}\\]\nConsider a node \\(x \\in A - \\{a\\}\\):\nsince \\(b\\) has entered partition \\(A\\), the internal cost of \\(x\\) increases by \\(c_{xb}\\).\nSimilarly, since \\(a\\) has entered the opposite partition \\(B\\); the internal cost of \\(x\\) must be decreased by \\(c_{ax}\\).\nThe new internal cost of \\(x\\) therefore is \\(I'_x = I_x - c_{xa} + c_{xb}\\)\nSince \\(b\\) has entered partition \\(A\\), the external cost of \\(x\\) decreases by \\(c_xb\\).\nSimilarly, since \\(a\\) has entered the opposite partition \\(B\\); the external cost of \\(x\\) must be increased by \\(c_{xa}\\).\nThe new external cost of \\(x\\) therefore is \\(E'_x = E_x + c_{xa} - c_{xb}\\)\nFor any node \\(x \\in A - \\{a\\}\\), we define updated \\(D\\) value as \\[D'_x = E'_x - I'_x\\] \\[D'_x = D_x + 2 c_{xa} - 2 c_{xb}\\]\nSimilarly, the new \\(D\\)-value of \\(y \\in B - \\{ b\\}\\) is \\[D'_y = E'_y - I'_y = D_y + 2c_{yb} - 2c_{ya}\\]\nNotice that if a module \\('x'\\) is neither connected to \\('a'\\) nor to \\('b'\\) then \\(c_{xa} = c_{xb} = 0, and, D'_x = D_x\\).\n\nOverview KL algorithm\n\nCompute \\(g_{ab}\\) for all \\(a\\in A\\) and \\(b \\in B\\).\nSelect the pair \\((a_1, b_1)\\) with maximum gain \\(g_1\\) and lock \\(a_1\\) and \\(b_1\\).\nUpdate the \\(D\\) values of remaining free nodes and re-compute the gains.\nThen a second pair \\((a_2, b_2)\\) with maximum gain \\(g_2\\) is selected and locked. Hence the gain of swapping the pair \\((a_1, b_1)\\) followed by the \\((a_2, b_2)\\) swap is \\(G_2 = g_1 + g_2\\).\nContinue selecting \\((a_3, b_3), ..., (a_i, b_i), ..., (a_n, b_n)\\) with gains \\(g_3, ..., g_i, ..., g_n\\).\nThe gain of making the swap of the first \\(k\\) pairs is \\(G_k = Eg_i\\). If there is no \\(k\\) such that \\(G_k &gt; 0\\) then the current partition cannot be improved; otherwise choose the \\(k\\) that maximizes \\(G_k\\), and make the interchange of \\(\\{a_1, a_2, ..., a_k\\}\\) with \\(\\{b_1, b_2, ..., b_k\\}\\) permanent.\n\n\n\nAlgorithm described in the slides\nThe algorithm is a simple, well-established heuristic to solve the partitioning problem.\nInput: Edge-weighted graph\nOutput: Bisection that minimizes cut-weight (or number of edges)\nKernighan-Lin maintains bisection from some start bisection and improves it in several steps until no further improvement can be found.\nPartition a network into two groups of predefined size. Arbitrary start partition.\nOne pass:\n\nInspect each a pair- s of nodes (one node from each group). Identify the pair that results in the largest reduction of the cut size if we swap them.\nSwap them. If no pair reduces the cut size, we can swap the pair that increases the cut size the least.\nThe process is repeated until each node is moved once (lock moved nodes).\nFind gain maximizing series (prefix) of steps - min cut cost encountered.\n\nRepeat until no gain possible.\nHow to calculate cost/gain? For unweighted (similar for weighted case):\nLet the internal degree \\(k_i^{int}\\) of node \\(i\\) be the number of links to other nodes in its partition \\(C\\), external degree \\(k_i^{ext}\\) number of links to the rest.\nCost reduction for moving \\(i\\): \\(D_i = k_i^{ext} - k_i^{int}\\)\nCost reduction (gain) for swapping \\(i\\) and \\(j\\): \\(g_{ij} = D_i + D_j - 2c_{ij}\\) where \\(c_{ij}\\) are the cost of links between \\(i\\) and \\(j\\) (double counted in the \\(D\\)s but don’t provide gain)\nUpdate of cost reduction values (for C, similar for other partition):\n\\(D'_x = D_x + 2c_{xi} - 2c_{xj}, x\\in C - \\{i\\}\\) (gain external and loose internal)\n(no change if not connected to \\(i, j\\))\nNote that swapping gain might be negative - densely connected subgraphs that need to be swapped as a whole for gain.\nRunning time (simple version): Gain computation quadratic, major cost, each node moded \\(= \\mathcal{O}(n^3)\\)\nInstead: Sort \\(D\\) values for each partition \\(\\mathcal{O}(n\\log n)\\). Maintain best gain \\(g_{max}\\) found so far. If need to compute gain \\(m_{lm} &lt; g_{max}\\) we don’t need to compute \\(g_{kp}\\) for \\(k&gt;l, p&gt;m\\) (if \\(c_{xy}\\) non-negative). \\(\\mathcal{O}(n^2 \\log n)\\).\nWith random start, results change - multiple runs.\nExtensions: * Swap minimum partition cardinality for unequal splits * K-way partitioning for more than two partitions. * Rather as improvement that stand-alone.\n\nsource\n\n\n\ngenerate_random_tree\n\n generate_random_tree (num_nodes, seed=None)\n\n\n# Generate three types of trees\nline_tree = nx.path_graph(n)  # Line/tree path\nstar_tree = nx.star_graph(n - 1)  # Center node 0, all others connect to it\nrandom_tree = generate_random_tree(n, seed=42)  # Random tree\n\n\nsource\n\n\ndraw_trees\n\n draw_trees (trees, titles)\n\n\n# Draw the trees\ndraw_trees([line_tree, star_tree, random_tree], [\"Line Tree\", \"Star Tree\", \"Random Tree\"])\n\n\n\n\n\n\n\n\n\n# Settings\nn_max = 10\nk_bar = 1.2\n\nfor n in range(10, n_max, 10):\n    m = int((k_bar * n) / 2)\n    G = nx.gnm_random_graph(n, m)\n\n    # Get the size of the largest component\n    largest_cc = max(nx.connected_components(G), key=len)\n    G_sub = G.subgraph(largest_cc)\n\n    # Plot largest component only\n    plt.figure(figsize=(5, 5))\n    pos = nx.spring_layout(G_sub)\n    nx.draw(G_sub, pos, node_color='lightblue', with_labels=False, node_size=30)\n    plt.title(f'n = {n}, k̄ = {k_bar}, giant component size = {len(G_sub)}')\n    plt.axis('off')\n    plt.show()\n\n\nfrom scipy.optimize import fsolve\n\n\nsource\n\n\ngiant_component_size\n\n giant_component_size (k)\n\n\nk_vals = np.linspace(0, 3, 300)\nS_vals = [giant_component_size(k) for k in k_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(k_vals, S_vals, label='Size of giant component $S$')\nplt.axvline(1, color='red', linestyle='--', label='Phase transition at $\\\\langle k \\\\rangle = 1$')\nplt.xlabel('Average degree $\\\\langle k \\\\rangle$')\nplt.ylabel('Giant component size $S$ (fraction of nodes)')\nplt.title('Emergence of the Giant Component')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_193901/2603444820.py:7: RuntimeWarning: The iteration is not making good progress, as measured by the \n improvement from the last ten iterations.\n  S_solution, = fsolve(func, S_guess)\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ngiant_component_size\n\n giant_component_size (k)\n\n\nsource\n\n\nupdate_graph\n\n update_graph (n=100, k_bar=0.5)",
    "crumbs": [
      "Lecture 06"
    ]
  },
  {
    "objectID": "netana_ex04.html",
    "href": "netana_ex04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "(2 points)\n\nExecute the Kernighan-Lin partitioning variant as given in the lecture in the following network, where the starting partitions are given by the colours.\nTry to find a starting partition that changes the outcome. Either provide it with the changed outcome or try to explain why it is not possible.\n\n\n\n\n\n\n\n\n\n\nTo provide a solution to the exercise: Provide the D values for each step and the maximum gain for the selection of the swap. Indicate the change of partitions from the swaps (preferred: visually), including the selected nodes, as well as the fixed nodes in each step. List the cut costs for each step and the selected prefix of swaps, too. Provide a starting partition or argument.\nGiven: A graph with \\(2n\\) nodes where each node has the same weight.\nGoal: A partition of the graph into two disjoint subsets \\(A\\) and \\(B\\) with minimum cut cost and \\(|A| = |B| = n\\).\nThe example has \\(n=4\\)\nWe define the reduction in cost for when two elements \\(a\\in A\\) and \\(b\\in B\\) are interchanged as \\(g_{ab} = D_a + D_b - 2c_{ab}\\) and the cost reduction for moving \\(a\\): \\(D_a = k_a^{ext} - k_a^{int}\\).\nFor the start we can define:\nCut cost: \\(9\\)\nNot fixed: \\(1, 2, 3, 4, 5, 6, 7, 8\\)\nBenefit \\(D_v\\) of each node:\n\nd = {\n    1: 1,\n    2: 1,\n    3: 2,\n    4: 1,\n    5: 1,\n    6: 2,\n    7: 1,\n    8: 1,\n}\n\nNow we need to compute the gains for all possible swaps and look for the maximum gain swap:\nFor that we also define \\(c_{ij}\\) as the cost of links between \\(i\\) and \\(j\\) (double counted in the \\(D\\)s but don’t provide gain)\n\n# Connection checker (returns 1 if connected, else 0)\ndef c(i, j):\n    vi, vj = g.vertex(i - 1), g.vertex(j - 1)  # 1-based to 0-based for graph_tool\n    return 1 if g.edge(vi, vj) or g.edge(vj, vi) else 0\n\ndef gain(i, j, d):\n    return d[i] + d[j] - 2 * c(i, j)\n\ndef compute_gains(d):\n    g = {}\n    for i in range(1, 5):\n        for j in range(5, 9):\n            g[(i, j)] = gain(i, j, d)\n    return g\n\n\ngains = compute_gains(d)\ngains\n\n{(1, 5): 0,\n (1, 6): 1,\n (1, 7): 2,\n (1, 8): 2,\n (2, 5): 0,\n (2, 6): 1,\n (2, 7): 2,\n (2, 8): 2,\n (3, 5): 3,\n (3, 6): 2,\n (3, 7): 1,\n (3, 8): 1,\n (4, 5): 2,\n (4, 6): 3,\n (4, 7): 0,\n (4, 8): 0}\n\n\n\ndef compute_max_gain(gains):\n    mx_gain = min(gains.values())\n    mx_gain_index = Tuple\n    for i in range(1, 5):\n        for j in range(5, 9):\n            if gains[(i,j)]&gt; mx_gain:\n                mx_gain = gains[(i,j)]\n                mx_gain_index = (i, j)\n    print (f\"maximum gain: {mx_gain}, nodes to swap: {mx_gain_index}\")\n\n    return mx_gain, mx_gain_index\n\n\nmax_gain, max_gain_index = compute_max_gain(gains)\n\nmaximum gain: 3, nodes to swap: (3, 5)\n\n\nNow we choose nodes (3, 5) for swapping and fix them, though we could also swap (4, 6).\n\ndef swap_nodes(i, j):\n    # Swap nodes 3 and 5\n    A.remove(i)\n    B.remove(j)\n    A.add(j)\n    B.add(i)\n\n    # Update vertex colors\n    for i in range(1, 9):\n        if i in A:\n            fill_color[g.vertex(i - 1)] = \"orange\"\n        else:\n            fill_color[g.vertex(i - 1)] = \"yellow\"\n\n    # Draw updated graph\n    graph_draw(\n        g, pos=pos, vertex_fill_color=fill_color,\n        vertex_text=labels, output_size=(400, 400)\n    )\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\nCut cost: \\(6\\)\nNot fixed: \\(1, 2, 4, 6, 7, 8\\)\nNew \\(D'_v\\)-values of each node is computed as \\[D'_x = D_x + 2c_{xa} - 2c_{xb}, \\forall x \\in A - \\{a\\}\\] \\[D'_y = D_y + 2c_{yb} - 2c_{ya}, \\forall y \\in B - \\{b\\}\\]\n\ndef new_d(x, a, b):\n    return d[x] + 2*c(x, a) - 2*c(x, b)\n\n\nd_new = {\n    1: new_d(1, 3, 5),\n    2: new_d(2, 3, 5),\n    3: -1000,\n    4: new_d(4, 3, 5),\n    5: -1000,\n    6: new_d(6, 5, 3),\n    7: new_d(7, 5, 3),\n    8: new_d(8, 5, 3),\n}\nd_new\n\n{1: -1, 2: -1, 3: -1000, 4: 3, 5: -1000, 6: 2, 7: -1, 8: -1}\n\n\n\ngains = compute_gains(d_new)\ngains\n\n{(1, 5): -1003,\n (1, 6): -1,\n (1, 7): -2,\n (1, 8): -2,\n (2, 5): -1003,\n (2, 6): -1,\n (2, 7): -2,\n (2, 8): -2,\n (3, 5): -2000,\n (3, 6): -1000,\n (3, 7): -1003,\n (3, 8): -1003,\n (4, 5): -997,\n (4, 6): 5,\n (4, 7): 0,\n (4, 8): 0}\n\n\n\nmax_gain, max_gain_index = compute_max_gain(gains)\n\nmaximum gain: 5, nodes to swap: (4, 6)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\n\nd_new_new = {\n    1: new_d(1, 4, 6),\n    2: new_d(2, 4, 6),\n    3: -1000,\n    4: -1000,\n    5: -1000,\n    6: -1000,\n    7: new_d(7, 6, 4),\n    8: new_d(8, 6, 4),\n}\nprint(d_new_new)\ngains = compute_gains(d_new_new)\nprint(gains)\nmax_gain, max_gain_index = compute_max_gain(gains)\n\n{1: -1, 2: -1, 3: -1000, 4: -1000, 5: -1000, 6: -1000, 7: -1, 8: -1}\n{(1, 5): -1003, (1, 6): -1003, (1, 7): -2, (1, 8): -2, (2, 5): -1003, (2, 6): -1003, (2, 7): -2, (2, 8): -2, (3, 5): -2000, (3, 6): -2002, (3, 7): -1003, (3, 8): -1003, (4, 5): -2000, (4, 6): -2000, (4, 7): -1003, (4, 8): -1003}\nmaximum gain: -2, nodes to swap: (1, 7)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\n\nd_new_new_new = {\n    1: -1000,\n    2: new_d(2, 4, 6),\n    3: -1000,\n    4: -1000,\n    5: -1000,\n    6: -1000,\n    7: -1000,\n    8: new_d(8, 6, 4),\n}\nprint(d_new_new_new)\ngains = compute_gains(d_new_new_new)\nprint(gains)\nmax_gain, max_gain_index = compute_max_gain(gains)\n\n{1: -1000, 2: -1, 3: -1000, 4: -1000, 5: -1000, 6: -1000, 7: -1000, 8: -1}\n{(1, 5): -2002, (1, 6): -2002, (1, 7): -2000, (1, 8): -1001, (2, 5): -1003, (2, 6): -1003, (2, 7): -1001, (2, 8): -2, (3, 5): -2000, (3, 6): -2002, (3, 7): -2002, (3, 8): -1003, (4, 5): -2000, (4, 6): -2000, (4, 7): -2002, (4, 8): -1003}\nmaximum gain: -2, nodes to swap: (2, 8)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\nHier muss man jetzt noch evaluieren wo man den besten gain hatte. Also eigentlich nur nachsehen wo die cut-size am niedrigsten war.\n\n\nThere is something about starting partitions given as colors. So probably we already have a preselection and we are now only optimizing this selection to get a better fit for the network.\nFinding a starting partition that changes the outcome, so select different colors that then result in a different partition just because of this starting set.\nWhat do i actually need to do in practice? So how do i actually produce valid results? Sounds like a lot of text, some numbers and essentially following through a set of predefined steps and reporting these metrics and my reasoning at each step.\n\\(D\\) Values\nThere are multiple steps as it seems in Kernighan-Lin\nMaximum Gain for the selection of the swap - whatever this means\nChange of partitions from the swaps -&gt; visually (so draw a picture)\nselected nodes\nfixed nodes in each step\nList cut costs for each step\nSelected prefix of swaps\nStarting partition and argument\nThe argument is obviously that no matter which starting arrangement is chosen you will always arrive at the optimal solution since you try out every possibility. The only version i could think of is one that basically lures a first swap that makes the optimal set impossible in the beginning and since these nodes are already locked in the beginning the optimal solution of cut size 1 can never be reached. This could be a possibility that we can look into for finding. But nonetheless it is otherwise very unlikely with this algorithm.\nThe required solution is exactly what is demonstrated in this Video",
    "crumbs": [
      "Assignment 4"
    ]
  },
  {
    "objectID": "netana_ex04.html#exercise-1",
    "href": "netana_ex04.html#exercise-1",
    "title": "Assignment 4",
    "section": "",
    "text": "(2 points)\n\nExecute the Kernighan-Lin partitioning variant as given in the lecture in the following network, where the starting partitions are given by the colours.\nTry to find a starting partition that changes the outcome. Either provide it with the changed outcome or try to explain why it is not possible.\n\n\n\n\n\n\n\n\n\n\nTo provide a solution to the exercise: Provide the D values for each step and the maximum gain for the selection of the swap. Indicate the change of partitions from the swaps (preferred: visually), including the selected nodes, as well as the fixed nodes in each step. List the cut costs for each step and the selected prefix of swaps, too. Provide a starting partition or argument.\nGiven: A graph with \\(2n\\) nodes where each node has the same weight.\nGoal: A partition of the graph into two disjoint subsets \\(A\\) and \\(B\\) with minimum cut cost and \\(|A| = |B| = n\\).\nThe example has \\(n=4\\)\nWe define the reduction in cost for when two elements \\(a\\in A\\) and \\(b\\in B\\) are interchanged as \\(g_{ab} = D_a + D_b - 2c_{ab}\\) and the cost reduction for moving \\(a\\): \\(D_a = k_a^{ext} - k_a^{int}\\).\nFor the start we can define:\nCut cost: \\(9\\)\nNot fixed: \\(1, 2, 3, 4, 5, 6, 7, 8\\)\nBenefit \\(D_v\\) of each node:\n\nd = {\n    1: 1,\n    2: 1,\n    3: 2,\n    4: 1,\n    5: 1,\n    6: 2,\n    7: 1,\n    8: 1,\n}\n\nNow we need to compute the gains for all possible swaps and look for the maximum gain swap:\nFor that we also define \\(c_{ij}\\) as the cost of links between \\(i\\) and \\(j\\) (double counted in the \\(D\\)s but don’t provide gain)\n\n# Connection checker (returns 1 if connected, else 0)\ndef c(i, j):\n    vi, vj = g.vertex(i - 1), g.vertex(j - 1)  # 1-based to 0-based for graph_tool\n    return 1 if g.edge(vi, vj) or g.edge(vj, vi) else 0\n\ndef gain(i, j, d):\n    return d[i] + d[j] - 2 * c(i, j)\n\ndef compute_gains(d):\n    g = {}\n    for i in range(1, 5):\n        for j in range(5, 9):\n            g[(i, j)] = gain(i, j, d)\n    return g\n\n\ngains = compute_gains(d)\ngains\n\n{(1, 5): 0,\n (1, 6): 1,\n (1, 7): 2,\n (1, 8): 2,\n (2, 5): 0,\n (2, 6): 1,\n (2, 7): 2,\n (2, 8): 2,\n (3, 5): 3,\n (3, 6): 2,\n (3, 7): 1,\n (3, 8): 1,\n (4, 5): 2,\n (4, 6): 3,\n (4, 7): 0,\n (4, 8): 0}\n\n\n\ndef compute_max_gain(gains):\n    mx_gain = min(gains.values())\n    mx_gain_index = Tuple\n    for i in range(1, 5):\n        for j in range(5, 9):\n            if gains[(i,j)]&gt; mx_gain:\n                mx_gain = gains[(i,j)]\n                mx_gain_index = (i, j)\n    print (f\"maximum gain: {mx_gain}, nodes to swap: {mx_gain_index}\")\n\n    return mx_gain, mx_gain_index\n\n\nmax_gain, max_gain_index = compute_max_gain(gains)\n\nmaximum gain: 3, nodes to swap: (3, 5)\n\n\nNow we choose nodes (3, 5) for swapping and fix them, though we could also swap (4, 6).\n\ndef swap_nodes(i, j):\n    # Swap nodes 3 and 5\n    A.remove(i)\n    B.remove(j)\n    A.add(j)\n    B.add(i)\n\n    # Update vertex colors\n    for i in range(1, 9):\n        if i in A:\n            fill_color[g.vertex(i - 1)] = \"orange\"\n        else:\n            fill_color[g.vertex(i - 1)] = \"yellow\"\n\n    # Draw updated graph\n    graph_draw(\n        g, pos=pos, vertex_fill_color=fill_color,\n        vertex_text=labels, output_size=(400, 400)\n    )\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\nCut cost: \\(6\\)\nNot fixed: \\(1, 2, 4, 6, 7, 8\\)\nNew \\(D'_v\\)-values of each node is computed as \\[D'_x = D_x + 2c_{xa} - 2c_{xb}, \\forall x \\in A - \\{a\\}\\] \\[D'_y = D_y + 2c_{yb} - 2c_{ya}, \\forall y \\in B - \\{b\\}\\]\n\ndef new_d(x, a, b):\n    return d[x] + 2*c(x, a) - 2*c(x, b)\n\n\nd_new = {\n    1: new_d(1, 3, 5),\n    2: new_d(2, 3, 5),\n    3: -1000,\n    4: new_d(4, 3, 5),\n    5: -1000,\n    6: new_d(6, 5, 3),\n    7: new_d(7, 5, 3),\n    8: new_d(8, 5, 3),\n}\nd_new\n\n{1: -1, 2: -1, 3: -1000, 4: 3, 5: -1000, 6: 2, 7: -1, 8: -1}\n\n\n\ngains = compute_gains(d_new)\ngains\n\n{(1, 5): -1003,\n (1, 6): -1,\n (1, 7): -2,\n (1, 8): -2,\n (2, 5): -1003,\n (2, 6): -1,\n (2, 7): -2,\n (2, 8): -2,\n (3, 5): -2000,\n (3, 6): -1000,\n (3, 7): -1003,\n (3, 8): -1003,\n (4, 5): -997,\n (4, 6): 5,\n (4, 7): 0,\n (4, 8): 0}\n\n\n\nmax_gain, max_gain_index = compute_max_gain(gains)\n\nmaximum gain: 5, nodes to swap: (4, 6)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\n\nd_new_new = {\n    1: new_d(1, 4, 6),\n    2: new_d(2, 4, 6),\n    3: -1000,\n    4: -1000,\n    5: -1000,\n    6: -1000,\n    7: new_d(7, 6, 4),\n    8: new_d(8, 6, 4),\n}\nprint(d_new_new)\ngains = compute_gains(d_new_new)\nprint(gains)\nmax_gain, max_gain_index = compute_max_gain(gains)\n\n{1: -1, 2: -1, 3: -1000, 4: -1000, 5: -1000, 6: -1000, 7: -1, 8: -1}\n{(1, 5): -1003, (1, 6): -1003, (1, 7): -2, (1, 8): -2, (2, 5): -1003, (2, 6): -1003, (2, 7): -2, (2, 8): -2, (3, 5): -2000, (3, 6): -2002, (3, 7): -1003, (3, 8): -1003, (4, 5): -2000, (4, 6): -2000, (4, 7): -1003, (4, 8): -1003}\nmaximum gain: -2, nodes to swap: (1, 7)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\n\nd_new_new_new = {\n    1: -1000,\n    2: new_d(2, 4, 6),\n    3: -1000,\n    4: -1000,\n    5: -1000,\n    6: -1000,\n    7: -1000,\n    8: new_d(8, 6, 4),\n}\nprint(d_new_new_new)\ngains = compute_gains(d_new_new_new)\nprint(gains)\nmax_gain, max_gain_index = compute_max_gain(gains)\n\n{1: -1000, 2: -1, 3: -1000, 4: -1000, 5: -1000, 6: -1000, 7: -1000, 8: -1}\n{(1, 5): -2002, (1, 6): -2002, (1, 7): -2000, (1, 8): -1001, (2, 5): -1003, (2, 6): -1003, (2, 7): -1001, (2, 8): -2, (3, 5): -2000, (3, 6): -2002, (3, 7): -2002, (3, 8): -1003, (4, 5): -2000, (4, 6): -2000, (4, 7): -2002, (4, 8): -1003}\nmaximum gain: -2, nodes to swap: (2, 8)\n\n\n\nswap_nodes(max_gain_index[0], max_gain_index[1])\n\n\n\n\n\n\n\n\nHier muss man jetzt noch evaluieren wo man den besten gain hatte. Also eigentlich nur nachsehen wo die cut-size am niedrigsten war.\n\n\nThere is something about starting partitions given as colors. So probably we already have a preselection and we are now only optimizing this selection to get a better fit for the network.\nFinding a starting partition that changes the outcome, so select different colors that then result in a different partition just because of this starting set.\nWhat do i actually need to do in practice? So how do i actually produce valid results? Sounds like a lot of text, some numbers and essentially following through a set of predefined steps and reporting these metrics and my reasoning at each step.\n\\(D\\) Values\nThere are multiple steps as it seems in Kernighan-Lin\nMaximum Gain for the selection of the swap - whatever this means\nChange of partitions from the swaps -&gt; visually (so draw a picture)\nselected nodes\nfixed nodes in each step\nList cut costs for each step\nSelected prefix of swaps\nStarting partition and argument\nThe argument is obviously that no matter which starting arrangement is chosen you will always arrive at the optimal solution since you try out every possibility. The only version i could think of is one that basically lures a first swap that makes the optimal set impossible in the beginning and since these nodes are already locked in the beginning the optimal solution of cut size 1 can never be reached. This could be a possibility that we can look into for finding. But nonetheless it is otherwise very unlikely with this algorithm.\nThe required solution is exactly what is demonstrated in this Video",
    "crumbs": [
      "Assignment 4"
    ]
  },
  {
    "objectID": "netana_ex04.html#exercise-2-modularity",
    "href": "netana_ex04.html#exercise-2-modularity",
    "title": "Assignment 4",
    "section": "Exercise 2 Modularity",
    "text": "Exercise 2 Modularity\n(3 points) * Experiment with modularity optimisation in your favorite software. Calculate modularity values (e.g. in networkx) for the “non-intuitive” example networks given in the lecture. * Optimise modularity for the following graph consisting of a ring of same-size cliques and neighbouring cliques connected via a single link. Determine the modularity of a clustering in which each clique is its own cluster, and of the clustering where two neighbouring cliques are forming a cluster. * Proof that a clustering with optimal modularity has no cluster with a single node of degree 1. * Extra(not mandatory): Given a line graph, a path of N nodes. Show that if we divide the network into two parts by cutting a line, with r and N-r nodes each part, then the modularity is \\[\\frac{3 − 4𝑛 + 4𝑟𝑛 − 4𝑟^2}{2(𝑛 − 1)^2}\\] Hence show that if N is even, the optimal division for modularity is splitting the graph exactly in the middle.\nTo provide a solution to the exercise: Provide the modularity values (a), the best modularity optimising community structure you can find (b) drawing, argument, and the proof(s).\nThis Exercise is already part of the community detection section. So Lecture 7 essentially. I first need to get that into my head before i can tackle that. ’",
    "crumbs": [
      "Assignment 4"
    ]
  },
  {
    "objectID": "netana_ex04.html#exercise-3",
    "href": "netana_ex04.html#exercise-3",
    "title": "Assignment 4",
    "section": "Exercise 3",
    "text": "Exercise 3\n(2 points) * Consider the following small network To provide a solution to the exercise: Calculate for each of the 10 pairs of nodes the cosine similarity. Based on the values, construct the dendrogram for the single-linkage hierarchical clustering of the network.\nOk the exercise is about hierarchical clustering\nClustering is also part of lecture 7 and comes after community detection. Clustering just has this special goal of having a similarity metric. So you are not just looking for communities based on minimizing the cut size between the communities and basically having this as your similarity metric. But you could have an arbitrary similarity metric that can also have to do with some of the additional data contained in the nodes. So this clustering is also something that is done in normal data analysis where you just have structured data. So graph clustering is like this but with the additional Network structure on top.",
    "crumbs": [
      "Assignment 4"
    ]
  },
  {
    "objectID": "netana_ex04.html#exercise-4",
    "href": "netana_ex04.html#exercise-4",
    "title": "Assignment 4",
    "section": "Exercise 4",
    "text": "Exercise 4\n(3 points) Clustering\nExecute partitioning/community detection/clustering on two networks of your chosing (but with more than 100 nodes).\nSee for example for networkx the documentation on communities to find supported methods, but feel free to use a different tool or framework (note that Leiden and Louvain are implemented).\n\nApply a partitioning / community detection method on your networks.\nApply a hierarchical clustering on your networks.\nDraw the networks with the partitions / communities mapped to visual variables (e.g. color). Report the images and shortly discuss if you think the results can be improved.\nDraw the dendrogram from the hierarchical clustering and report it.\nFind a level in the dendrogram that you consider a good clustering. Report it and shortly explain why it is better than the others.",
    "crumbs": [
      "Assignment 4"
    ]
  },
  {
    "objectID": "netana_l07.html",
    "href": "netana_l07.html",
    "title": "Lecture 07",
    "section": "",
    "text": "from graph_tool import Graph\n#| hide\nfrom nbdev.showdoc import *\nimport numpy as np\nfrom graph_tool import Graph\n# L is Laplacian Matrix\n# D is Diagonal Degree Matrix\nSo basically in the last lecture we talked about network partitioning which separated a network into \\(K\\) equal size parts while minimizing the cut size.\nThe question that is now obvious is what happens if we don’t want balanced partitions because it wouldn’t represent the structure in our network well.\nPartitioning forces a structure upon our network that might or might not be there essentially. But we would rather want to find out what the internal grouping structure of a network, without making assumptions about it beforehand.\nThis is where community detection comes in.",
    "crumbs": [
      "Lecture 07"
    ]
  },
  {
    "objectID": "netana_l07.html#community",
    "href": "netana_l07.html#community",
    "title": "Lecture 07",
    "section": "Community",
    "text": "Community\n\nThis section tells us how to qualify what a community is, how it differs from other groupings like partitions or node roles, introduces different types of detection methods, discusses community detection as a hard problem, and gives an example of how challenging and relevant this is at scale.\n\nA community is a group of nodes that is formed by mutual adjacency, proximity, or reachability. It suggests the presence of real-world interaction for a common purpose.\nNodes in a community are more likely to connect to each other than to nodes from other communities.\nWe can capture this by: Given connected subgraph \\(C\\), let again the internal degree \\(d_i^{int}\\) of node \\(i\\) be the number of links to other nodes in \\(C\\), external degree \\(k_i^{ext}\\) number of links to the rest.\nThen \\(C\\) is a strong community if each node in \\(C\\) has \\(k_i^{int}(C) &gt; k_i^{ext}(C)\\)\n\\(C\\) is a weak community if \\(\\sum_{i\\in C} k_i^{int}(C) &gt; \\sum_{i\\in C}k_i^{ext}(C)\\)\nE.g. each clique is a strong community\nNote the difference to\n\nPartition - we do not assume predefined numbers or sizes of groups, or non-overlap\nNodes with similar roles - specific characteristics or function of single nodes in a network (e.g. peripheral, center of a star, member of a clique). These nodes belong to a category but don’t necessarily form a community.\n\nAn equivalence relation of the vertices corresponds to a partitioning (into equivalent classes) and a role assignment, not communities.\nExample: Network from data of belgian mobile phone company. 2.6 million users, calls over 6 months. Blondel et al.’s analysis reveals 261 communities of &gt;100 customers (node size proportional to #customers, color language spoken, red French, green Dutch\nOur goal here will be to find algorithms that can detect the presence and the size and type of each of these communities in our network.",
    "crumbs": [
      "Lecture 07"
    ]
  },
  {
    "objectID": "netana_l07.html#community-detection",
    "href": "netana_l07.html#community-detection",
    "title": "Lecture 07",
    "section": "Community Detection",
    "text": "Community Detection\n\nHow to Identify Communities?\n\nBeyond the term Community there is also a mayor problem of detecting communities in networks. This is not easy and not every network does necessarily have a community. So algorithms have to be able to detect them if they are there but also be able to say that the arent. It seems like there are two mayor methods. They are handeled in two subsections, namely Modularity and Divisive, which seem to be just two different methods for doing this.\n\nCommunity detection: Major research topic\nMany definitions of criteria and many methods exist.\n\nTwo main aspects: An objective and a method to optimise it.\nParadigms to build communities:\n\nAgglomerative: start from single entities, increase\nDivisive: Start with full graph, remove edges, e.g. based on the importance of edges.\n\nSometimes we have additional constraints, e.g. the number of groups.\nMany goals constitute hard optimization problems, and thus many heuristics and approximation methods exist.\nNote: There simply might not be good communities in a graph (Question: How do we know?).\nWhat about the number of possible solutions here?\nGiven by Bell number, recursively \\(B_{N+1} = \\sum_{k=0}^N B_k, B_0 = 1\\)\nDobinski formula \\(B_{N+1} = \\frac{1}{e}\\sum_{k=0}^\\inf \\frac{k^N}{k!}\\)\n1, 1, 2, 5, 15, 52, 203, 877, 4140, 21147, 115975, 678570, 4213597, 27644437,…\n\nCommunity Detection Measures — Modularity\nModularity: An objective for community detection. Measures the extent to which like is connected to like (within a community) for a given partitioning in communities.\nStrictly less than one, positive if there are more “like-to-like” edges than expected by random chance (configuration model). Negative values \\((\\geq-0.5)\\) indicate disassortative mixing. \\[Q = \\frac{1}{\\underbrace{2M}_{\\text{Normalise}}}\\sum_{ij}\\left(\\underbrace{A_{ij}}_{\\text{Actual eges}}-\\frac{k_ik_j}{\\underbrace{2M}_{\\text{Expected edges}}}\\right)\\underbrace{\\delta_{g_ig_j}}_{\\text{Given partitioning\\ in communities }g_k}\\]\nNote that \\(\\sum_{\\text{edges}(i,j)}\\delta_{g_ig_j} = \\frac{1}{2}\\sum g_{ij}A_{ij} \\delta_{g_ig_j}\\) is the number of edges between nodes in the same community.\nExpected connection in a random null model with degree distribution \\(\\frac{k_i k_j}{2M}\\) as we have \\(k_i\\) opportunities to end an edge from i that one of the stubs at j, which have a fraction of \\(\\frac{k_j}{2M}\\) of all stubs.\nSometimes the equivalent community-only formula is used \\[Q = \\frac{1}{2M}\\sum_c \\left( E_c - \\gamma \\frac{K_c^2}{2M}\\right)\\]\nwith \\(E_c\\) edges in community \\(c, K_c\\) sum of degrees of nodes in \\(c\\), and \\(\\gamma\\) an additional resolution parameter. Communities should have at least density \\(\\gamma\\) while between them, it should be lower. Higher \\(\\gamma\\) leads to more communities. See e.g. networkx implementation.\n(Note that we can use this formula for easier optimisation / computation - merge edge lists.)\n\nMaximum modularity seems to work well for communities.\nIt can be used to evaluate quality of results from community detection algorithms or as objective for an algorithm.\nExamples for Optimal Partition with high \\(M\\), suboptimal partition with low \\(M\\), single community with \\(M=0\\), negative modularity with negative \\(M\\).\nMaximum modularity is not always intuitive, examples with non-local behaviour, a clique \\(K_3\\) with leaves, scaling behaviour, clusters represented as colours.\nNote: Similarity to assortativity, also finds communities in null model networks (model ignores search for communities in statistical manner - overfitting, finds high modularity communities, p-hacking)\nResolution limit max. \\(\\sqrt{2E}\\) communities: Prefers communities above a certain size (underfitting).\n\n\n\nModularity Methods\n\nCommunity Detection — Agglomerative\nMaximum modularity problem is unfortunately NP-complete. Simple heuristic: Greedy joining\nIteratively joins nodes if the move increases the new partition’s modularity (\\(N-1\\) merges, \\(\\mathcal{O}(N^2 log N)\\) — similar to our KL partitioning, up to engineering).\n\nStep 1: Assign each node a community of its own. Hence we start with \\(N\\) communities.\nStep 2: Inspect each pair of communities connected by at least one link and compute the modularity variation obtained if we merge these two communities.\nStep 3: Identify the community paris for which \\(\\Delta M\\) is the largest and merge them. Note that modularity of a particular partition is always calculated from the full topology of the network.\nStep 4: Repeat step 2 until all nodes are merged into a single community.\nStep 5: Record for each step and select the partition for which the modularity is maximal.\n\nUnbounded approximation ratio.\nCompromise of very fast and reasonable quality: Louvrain method (Bondel et al.) Agglomerative multilevel method.\nConsider individual node moves to different communities until no improvement is possible (Consider neighbours community). After initial merging, create new network with communities as nodes and repeat. Depending on structure reported to be somewhere between \\(\\mathcal{O}(N \\log N)\\) and \\(\\mathcal{O}(N^2)\\) in practice.\nProblem: Communities don’t need to be connected \\(\\rightarrow\\) Improvement Leiden algorithm: Refinement Step\nModularity Take-away\n\nConcept and formula\nHeuristics\n\nProceed with caution - it looks simple and interpretable with reasonable null model comparison, but results might be far from the expected. However, you will find a lot of methods in many tools, thus our discussion of it.\n\n\nCommunity Detection — Divisive\nCreate communities by iteratively removing edges that connect nodes with low similarity.\nCreates a hierarchy.\nE.g. Girvan-Newman:\n\nStep 1: Define a centrality measure for links\n\nLink betweenness is the number of shortest paths between all node paris that run along a link (relative to all). \\(\\mathcal{O}(MN)\\) or for sparse \\(\\mathcal{O}(N^2)\\)\nRandom-walk betweenness. A pair of nodes \\(m\\) and \\(n\\) are chosen at random. A walker starts at \\(m\\), following each adjacent link with equal probability unitl it reaches \\(n\\). Random walk betweenness \\(x_{ij}\\) is the probability that the link \\(i \\rightarrow j\\) was crossed by the walker after averaging over all possible choices for starting nodes \\(m\\) and \\(n\\) \\(\\mathcal{O}(MN^2)\\)\n\nStep 2: Hierarchical Clustering\n\nCompute the centrality of each link\nRemove the link with the largest centrality; in case of a tie, choose one randomly (!bad).\nRecalculate the centrality of each link for the altered network.\nRepeat until all links are removed (yields a dendogram).\n\n\nComputational complexity:\n\nStep 1a (calculation betweenness centrality): \\(\\mathcal{O}(N^2)\\) for sparse networks\nStep 1b (Recalculation of betweenness \\(\\mathcal{O}(N^3)\\) centrality for all links: \\(\\mathcal{O}(LN^2)\\)\n\n\n\nHow does the Divisive Method for community detection work?\n\nWhat is the process of finding communities in network data?\nWhat is modularity? How is it defined?\n\n\n# What kind of code can we write about community detection? We can probably load any graph and check if there is a community present in it and then get the characteristic metrics for these communities that we can then report on.\n\ndef find_community(graph: Graph):\n    return {}\n\ndef report_communities(communities):\n    pass\n\n\nG = Graph()\n\ncommunity = find_community(G) # so something like this could be interesting.\nreport_communities(community) # And then using this community data structure we could maybe report on it.\n# Now i definately don't know if this works this way, at least a good guess is that something exists here that we can report on and that we can acutally do in code and probably also usign functions that are already implemented either in graph_tool or networkx.",
    "crumbs": [
      "Lecture 07"
    ]
  },
  {
    "objectID": "netana_l07.html#clustering",
    "href": "netana_l07.html#clustering",
    "title": "Lecture 07",
    "section": "Clustering",
    "text": "Clustering\n\nClustering groups data points from a data set into subsets (clusters) that are homogeneous (compared to the rest - adding points might change grouping!)\nIn data science/machine learning is often considered unsupervised learning\nRequire some notion of similarity\nUsually similarity matrix \\(S\\) (or dissimilarity/distance)\n\n\nDefine some distance / similarity\nApply an optimisation that optimises an objective function, e.g. “close” points in clusters (e.g. k-means)\n\n\nGraph clustering partitions the nodes (usually) of a graph into clusters, i.e. node sets. Clusters might be disjoint, overlapping, or nested.\nOften: goal of having many intra-cluster edges and few inter-cluster edges. The most popular approach for graph clustering is extraction of tightly connected subgroups of nodes (which we termed community detection)\nMany graph clustering techniques using this approach are based on agglomerative clustering algorithms.\nOften clustering in general form is less focused on structure, rather attributes (cluster might be disconnected in the network model — but you might argue there is another network on the attributes) and without the assumption that clusters are (more) densely connected and sparse connectivity in between.\nIn practice also clustering of multivariate data / attributed graphs where topological structure is only a part.\nExamples: Clustering based on density of connections vs. clustering based on commonality of neighbors\n\nRelation Clustering vs. Community Detection\n\nSeveral sources: Clustering \\(\\approx\\) Community detection\nSeveral sources: Clustering \\(\\neq\\) Community detection\nSeveral sources: Clustering \\(\\subset\\) Community detection (e.g. Newman)\nSeveral sources: Clustering \\(\\supset\\) Community detection (e.g. Bader et al.)\n\nSurely strong overlap and roots/focus in, ah, different communities. E.g. Edge betweenness clustering - Girvan-Newman Note: We already encountered a reference to clustering, the clustering coeficient.\n\nHierarchical Clustering\nHierarchical clustering does not create simply a partitioning, but a multilevel tree structure of partitions, where each cluster except for the root is a subcluster of a supercluster.\n\nRoot cluster (contains all points)\nLevels cut through the hierarchy and constitute clusterings (not necessarily horizontal)\nAt the bottom data elements / nodes\n\nFrom bottom to top the cluster distance can be graphed in a dendogram.\nClustered graph \\[C = \\left(\\underbrace{G}_{\\text{Graph}}, \\underbrace{T}_{\\text{Inclusion Hierarchy}}\\right)\\]\nClustered drawing: Disjoint regions for disjoint clusters, representing the hierarchy. Note: In such an inclusion hierarchy, we lose the information on the cluster distance.\n\nAgain, we could proceed in agglomerative or divisive fashion.\nThe similarities can be given as input or computed based on input data / structure\nMany methods / classes of methods existing\n\nSimilarity:\n\nStructural equivalence — share neighbours\nRegular equivalence — have neighbors that are similar\n\n\nStructural Similarity\nMeasures:\n\nCommon neighbors \\(n_ij = \\sum_k A_{ik} A_{kj}\\) - this disregards the nodes’ degrees\nJaccard coefficient - normalize \\(n_{ij}\\) betwen 0 and 1 by dividing by total number of distinct neighbors \\(J_{ij} = \\frac{n_{ij}}{k_i+k_j-n_ij}\\) (count common neighbors once)\nCosine — defined on vectors:\nDot product \\(x \\cdot y = |x||y|\\cos\\theta \\rightarrow \\cos \\theta = \\frac{x\\cdot y}{|x||y|}\\)\n\\(0\\) if orthogonal / independent\nHere: Regard rows of adjacency matrix \\(A\\) as vectors and use cosine as similarity Dot product for undirected graphs simply \\(n_{ij}\\) for \\(i\\) and \\(j\\). Thus similarity \\[\\sigma_{ij} = \\cos \\theta = \\frac{\\sum_k A_{ik} A_{kj}}{\\sqrt{\\sum_k A_{ik}^2}\\cdot \\sqrt{\\sum_k A_{jk}^2}}\\] Note that for simple unweighted graphs \\(A_{ik}^l = A_{ik}\\) (only 0 or 1), \\(\\sum A_{ik} = k_i\\) (degree of i). Thus \\[\\sigma_{ij} = \\frac{n_{ij}}{\\sqrt{k_i k_j}}\\]\n\nThen similarity of i and j is number of common neighbours divided by geometric mean of degrees. (0 by convention for degree zero).\nValue of 0 for no sharing, 1 for the exact same neighbors.\n\n\n\n\nSAHN\nSequential agglomerative hierarchical non-overlapping (SAHN) clustering techniques belong to the classical clustering methods applied heavily in many application domains.\n\nAssign each node to its own cluster (N cluster)\nIteratively: In each step, decrease number of clusters by one by merging two “most similar” clusters.\n\nFinish either at the desired number of clusters or when only one cluster left.\n\nFind similar nodes: (Dis)Similarity / Distance matrix\n\\(S_{ij}\\) contains distance value from node \\(i\\) to node \\(j\\)\nMerge similar clusters: Need inter-cluster distance = linkage\n\nTypical linkage strategies (similarity between groups)\n\nSingle linkage - use smallest distance / dissimilarity (“merge by nearest neighbours”). Might create stretches.\nComplete linkage - use largest distance / dissimilarity (“merge by farthest neighbours”).\nAverage linkage - use average of all pairs of nodes (weighted average after merging)\n\nRavasz algorithm 1) Similarity: connect nodes that share many neighbours\nTopology overlap matrix \\[\\sigma_{ij} = \\frac{J(i,j)}{min(k_i, k_j) + 1 - \\Theta(A_{ij})}\\] \\(J(i, j)\\) simply is \\(n_{ij}\\) plus 1 in case of a direct link (i, j are not common neighbors) \\[\\Theta(A_{ij}) =\n   \\begin{cases}\n        0: A_{ij} \\leq 0 \\\\\n        1: \\text{else}\\\\\n    \\end{cases}\\] \\[\\sigma_{ij}=\n   \\begin{cases}\n        0: \\text{no direct link or common neighbors}\\\\\n        1: \\text{direct link and same neighbors}\\\\\n    \\end{cases}\\] 2) Merging: what are similarities between groups? Average Linkage 3) Building the hierarchy: 1) Each node is a singleton community 2) Find the pair with the highest similarity and merge 3) Calculate similarities for a new community 4) Repeat from 2) until a single community is left. 4) Build dendrogram, extract organization structure\nRunning Time:\n\nStep 1 (calculation similarity matrix): \\(\\mathcal{O}(N^2)\\)\nStep 2-3 (group similarity): \\(\\mathcal{O}(N^2)\\)\nStep 4 (dendrogram): \\(\\mathcal{O}(N log N)\\) Overall \\(\\mathcal{O}(N^2)\\)\n\nExample: Topological overlap matrix reordered to show high overlap.\n\nSimilar to the comparison of graphs with the random graph model, we can compare hierarchical structure by a hierarchical random graph model.\nSpecify a dendrogram (binary tree) and a set of probabilities at each intersection of the tree.\nThe probability for an edge between a pair of nodes is equal to the \\(p\\) stored at the lowest common ancestor.\nParameters are the probabilities and the tree structure\n\nAssumption in hierarchical clustering: Small modules are nested in larger ones. This is captured by the dendrogram. But does this faithfully reflect an organisational structure in the network or just and artifact of the approach? There are obviously networks with such a structure.\n\n\nHierarchical Netwok Model\n\nStart with a fully connected module, e.g. five nodes \\(K_5\\)\nCreate four copies and connect “peripheral” nodes to “central” nodes of original, e.g. 25 nodes\nRepeat from 2): 4 copies and connected peripheral to original central nodes. (125 nodes, …)\n\nScale-free property The obtained network is scale-free, its degree distribution following a power-law with \\(k^\\gamma\\). \\[\\gamma = 1 + \\frac{\\ln 5}{\\ln 4} \\simeq 2.16\\]\nExample:\n\nLargest hub: Starts with degree 4\n2nd step: add 4x4\n3rd step: add 4x4x4\nI.e. \\(4^n\\) per iteration\n\nAfter iteration n: \\(k_n(H_i) = \\sum_{l=1}^i 4^l\\) for hub on level \\(i\\)\n(there are four copies of the central nodes with degree from last iteration and so on)\nSmall \\(k\\) nodes:\n\nhigh clustering coefficient\ntheir neighbours tend to link to each other in highly interlinked and compact communities.\n\nHigh \\(k\\) nodes (hubs):\n\nsmall clustering coefficient\nConnect independent communities.\n\nGreen circles: Random rewiring\n\n\nWhat is Louvrain\nLouvrain Community Detection algorithm\n\n\nWhat is Leiden\nLeiden Community Detection algorithm\nextracting the community structure of a network based on modularity optimization.\nGuarantees that communities are well connected it is faster it uncovers better partitions\n\nWhat is a cluster? How is it defined? I think clustering is a whole nother problem again. I could read up on it if there is a similarity or connection between a cluster and a community.\nWhat is a motif? How is it defined? I think motif is not part of this lecture maybe the next one, at least i didn’t see the word anywhere unitl now.\nI need to get a handle of the formulas, not only to memorize them but to get a pattern recognition working for me, so i can find things again and again. Currently i am only pattern matching. Probably also because the slides dont seem to follow an understandable order. I should rather read the books but i also want to get what he tells us on the slides.",
    "crumbs": [
      "Lecture 07"
    ]
  },
  {
    "objectID": "netana_l01.html",
    "href": "netana_l01.html",
    "title": "Lecture 01",
    "section": "",
    "text": "What are networks, so all the fundamentals one might want to know about networks.",
    "crumbs": [
      "Lecture 01"
    ]
  },
  {
    "objectID": "netana_l01.html#networks",
    "href": "netana_l01.html#networks",
    "title": "Lecture 01",
    "section": "",
    "text": "What are networks, so all the fundamentals one might want to know about networks.",
    "crumbs": [
      "Lecture 01"
    ]
  },
  {
    "objectID": "netana_l01.html#networks-in-the-news",
    "href": "netana_l01.html#networks-in-the-news",
    "title": "Lecture 01",
    "section": "Networks in the News",
    "text": "Networks in the News\nHere we collect a number of sources where you find Networks be relevant to in the wild. So real-world incidents where we as network analysts could have something to say about what happened and what can be observed there and why. He collects some of these for us but it might be fun to be on the lookout when looking at the news ourselves.",
    "crumbs": [
      "Lecture 01"
    ]
  },
  {
    "objectID": "netana_l01.html#applications-of-networks",
    "href": "netana_l01.html#applications-of-networks",
    "title": "Lecture 01",
    "section": "Applications of Networks",
    "text": "Applications of Networks\nSome areas where networks are relevant. These are described in great detail in the Newman Book, so for more info we can also look there he just uses Criminalistics and Biological networks as some of his examples when talking about networks. So knowing about these application areas is predominantly important because it gives us the chance to actually understand the examples better and relate to why some of the metrics are important.",
    "crumbs": [
      "Lecture 01"
    ]
  },
  {
    "objectID": "netana_ex03.html",
    "href": "netana_ex03.html",
    "title": "Exercise 3",
    "section": "",
    "text": "(2 points) The tables show function values for functions \\(f(x)\\) and \\(g(x)\\) on different values of \\(x\\). Plot the function values in a way that allows you to determine if any of the functions exhibits power-law behavior. If yes, determine the exponent.\n\n\n\n\\(x\\)\n\\(f(x)\\)\n\n\n\n\n5.00\n252822.43\n\n\n7.50\n84599.77\n\n\n11.25\n28308.89\n\n\n16.88\n9472.76\n\n\n25.31\n3169.79\n\n\n37.97\n1060.68\n\n\n56.95\n354.93\n\n\n85.43\n118.77\n\n\n128.14\n39.74\n\n\n192.22\n13.30\n\n\n\n\n\n\n\\(x\\)\n\\(g(x)\\)\n\n\n\n\n3.00\n5824779.30\n\n\n5.10\n1185913.90\n\n\n8.67\n347172.77\n\n\n14.74\n114200.30\n\n\n25.06\n40746.59\n\n\n42.60\n17235.83\n\n\n72.41\n69248.85\n\n\n123.10\n384.66\n\n\n209.27\n10.41\n\n\n355.76\n0.13\n\n\n\n\n\nTo provide a solution to the exercise: Provide your plot, report your judgment on power-law behavior and in case the exponent.\nData plotted for \\(f(x)\\) and \\(g(x)\\):\n\n\n\n\n\n\n\n\n\nPower law is defined with \\(P_k = Ck^\\alpha\\).\nValues for alpha are often in the range 2-3 and taking the log of function and argument leads to a linear relation, visible as a diagonal straight line in a log-log plot.\n\n\n\n\n\n\n\n\n\nThis can be seen here where we have a straight line for \\(f(x)\\) but not for \\(g(x)\\) which suggests that we only have a power-law for \\(f(x)\\). But the lecture notes, that looking at the diagram alone might be deceiving. In particular, if the data is incomplete. True power law should be monotonically decreasing and node degree might deviate for small k.\nSo to validate this further, we can show that a function has power law behavior if it satisfies \\[f(x) \\sim x^{-\\alpha}\\] \\[f(x) = C \\cdot x^\\alpha\\] So taking logarithms gives us a linear relationship: \\[\\log(f(x)) = -\\alpha \\cdot \\log(x) + \\log(C)\\] We can estimate the exponent \\(\\alpha\\) using linear regression on the log-transformed data.\n\nsource\n\n\n\n\n plot_linregress_comparison (x, y, name, ax)\n\n\n\nEstimated power-law exponent f(x): α ≈ 2.70\nR-squared f(x): 1.000000\nEstimated power-law exponent g(x): α ≈ 3.16\nR-squared g(x): 0.850754\n\n\n\n\n\n\n\n\n\nThe estimated slope from the regression of \\(f(x)\\) is -\\(\\alpha\\), giving us \\(\\alpha \\approx 2.70\\)\nThe \\(R^2\\) value of the regression is \\(1.0\\), indicating a strong fit.\nFor \\(g(x)\\) we see that it does not follow a clear power-law pattern based on the regression and the low \\(R^2\\) value.",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "netana_ex03.html#power-law-behaviour",
    "href": "netana_ex03.html#power-law-behaviour",
    "title": "Exercise 3",
    "section": "",
    "text": "(2 points) The tables show function values for functions \\(f(x)\\) and \\(g(x)\\) on different values of \\(x\\). Plot the function values in a way that allows you to determine if any of the functions exhibits power-law behavior. If yes, determine the exponent.\n\n\n\n\\(x\\)\n\\(f(x)\\)\n\n\n\n\n5.00\n252822.43\n\n\n7.50\n84599.77\n\n\n11.25\n28308.89\n\n\n16.88\n9472.76\n\n\n25.31\n3169.79\n\n\n37.97\n1060.68\n\n\n56.95\n354.93\n\n\n85.43\n118.77\n\n\n128.14\n39.74\n\n\n192.22\n13.30\n\n\n\n\n\n\n\\(x\\)\n\\(g(x)\\)\n\n\n\n\n3.00\n5824779.30\n\n\n5.10\n1185913.90\n\n\n8.67\n347172.77\n\n\n14.74\n114200.30\n\n\n25.06\n40746.59\n\n\n42.60\n17235.83\n\n\n72.41\n69248.85\n\n\n123.10\n384.66\n\n\n209.27\n10.41\n\n\n355.76\n0.13\n\n\n\n\n\nTo provide a solution to the exercise: Provide your plot, report your judgment on power-law behavior and in case the exponent.\nData plotted for \\(f(x)\\) and \\(g(x)\\):\n\n\n\n\n\n\n\n\n\nPower law is defined with \\(P_k = Ck^\\alpha\\).\nValues for alpha are often in the range 2-3 and taking the log of function and argument leads to a linear relation, visible as a diagonal straight line in a log-log plot.\n\n\n\n\n\n\n\n\n\nThis can be seen here where we have a straight line for \\(f(x)\\) but not for \\(g(x)\\) which suggests that we only have a power-law for \\(f(x)\\). But the lecture notes, that looking at the diagram alone might be deceiving. In particular, if the data is incomplete. True power law should be monotonically decreasing and node degree might deviate for small k.\nSo to validate this further, we can show that a function has power law behavior if it satisfies \\[f(x) \\sim x^{-\\alpha}\\] \\[f(x) = C \\cdot x^\\alpha\\] So taking logarithms gives us a linear relationship: \\[\\log(f(x)) = -\\alpha \\cdot \\log(x) + \\log(C)\\] We can estimate the exponent \\(\\alpha\\) using linear regression on the log-transformed data.\n\nsource\n\n\n\n\n plot_linregress_comparison (x, y, name, ax)\n\n\n\nEstimated power-law exponent f(x): α ≈ 2.70\nR-squared f(x): 1.000000\nEstimated power-law exponent g(x): α ≈ 3.16\nR-squared g(x): 0.850754\n\n\n\n\n\n\n\n\n\nThe estimated slope from the regression of \\(f(x)\\) is -\\(\\alpha\\), giving us \\(\\alpha \\approx 2.70\\)\nThe \\(R^2\\) value of the regression is \\(1.0\\), indicating a strong fit.\nFor \\(g(x)\\) we see that it does not follow a clear power-law pattern based on the regression and the low \\(R^2\\) value.",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "netana_ex03.html#categorizing-networks",
    "href": "netana_ex03.html#categorizing-networks",
    "title": "Exercise 3",
    "section": "2) Categorizing Networks",
    "text": "2) Categorizing Networks\n(2 points) In the lecture we discussed a simple way to quantify assortativity and to assign a network into one of the three categories by fitting a power function to the degree correlation \\(\\bar k_{nn}(k)\\). Try this approach for the networks As3G1.txt and As3G2.txt in Ilias and report your results. Note this correlation is implemented (e.g. in networkx as average_degree_connectivity) A more formally sound version is to simply use the Pearson correlation coefficient \\(r\\) of the degrees at either ends of the graph’s edges. As it is normalized, it lies between \\(-1≤r≤1\\) and can be interpreted the same way as our correlation exponent. \\[r = \\frac{1}{\\sigma^2_q}\\sum_{jk}jk(e_{jk}-q_jq_k)\\] with \\(\\sigma_q^2\\) being the variance of excess degree distribution \\(q(k)\\). Luckily, this measure is also already implemented in many analysis tools (e.g. networkx as degree_pearson_correlation_coefficient). Apply the method on the network, report your result and compare to the finding for the first method. Then run a number (e.g. 10) of degree sequence preserving randomization and report if/which changes occur to the second measure.\n\nSolution\nTo provide a solution to the exercise: Provide the diagram for the fitting, and your correlation exponent, the result for pearson correlation for the original and randomised networks.\nThe networks are stored as edge lists. As3G2 additionally has edge weights. So first we need to load them:\n\nG1 = nx.read_adjlist(\"../data/As3G1.txt\", nodetype=int)\nprint(len(G1.edges))\nprint(len(G1.nodes))\nG2 = nx.Graph()\nwith open(\"../data/As3G2.txt\") as f:\n    for line in f:\n        a, b, attr = line.strip().split(maxsplit=2)\n        G2.add_edge(a, b, **ast.literal_eval(attr))\nprint(len(G2.edges))\nprint(len(G2.nodes))\n\n565\n430\n254\n77\n\n\nTo quantify the assortativity of the network we solve \\(\\bar k_{nn} = ak^\\mu\\) for the correlation exponent \\(\\mu\\) and categorize it. * \\(μ&gt;0\\): assortative (high-degree nodes link to high-degree) * \\(μ&lt;0\\): disassortative * \\(μ≈0\\): neutral\n\nsource\n\n\npower_law\n\n power_law (x, a, b)\n\nWe compute the average degree connectivity and get a dictionary keyed by degree k with the value of average connectivity.\nWe can fit a line on these values to get an estimation of our correlation exponent.\n\nsource\n\n\nfit_degree_connectivity\n\n fit_degree_connectivity (graph, title)\n\n\nsource\n\n\ncategorize_assortativity\n\n categorize_assortativity (mu, name)\n\n\nmu_G1 = fit_degree_connectivity(G1, \"G1: Degree Correlation Fit\")\n\nmu_G2 = fit_degree_connectivity(G2, \"G2: Degree Correlation Fit\")\ncategorize_assortativity(mu_G1, \"G1\")\n\ncategorize_assortativity(mu_G2, \"G2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe calculated exponentv for G1: 0.523 shows a assortative degree correlation.\nThe calculated exponentv for G2: -0.145 shows a disassortative degree correlation.\n\n\nNext we should compare the average_degree_connectivity with the degree_pearson_correlation_coefficient results which returns the assortativity of graph by degree.\n\n# compare the results\npearson_G1 = nx.degree_pearson_correlation_coefficient(G1)\npearson_G2 = nx.degree_pearson_correlation_coefficient(G2)\n\nprint(f\"G1 Pearson: {pearson_G1:.4f}, G2 Pearson: {pearson_G2:.4f}\")\n\nG1 Pearson: 0.7332, G2 Pearson: -0.1652\n\n\nHere, \\(G1\\) is assortative and \\(G2\\) is disassortative.\nNext we should run 10 degree sequence preserving randomizations and show which changes occur in the second measure. We do this by swapping edges in the network.\n\nIf the randomized networks lose any assortativity, that suggests the observed structure is meaningful.\nIf the Pearson correlation stays similar → the degree sequence alone explains assortativity.\n\nIn the results we can see that for \\(G1\\) the assortativity clearly changes to negative or very small values suggesting that the structure is meaningful.\nFor \\(G1\\) there is no such change visible.\n\nsource\n\n\nrandomize_and_measure\n\n randomize_and_measure (graph, n_iter=10)\n\n\n\nRandomized Pearson G1: [0.004137654023765612, -0.0729776597452127, -0.05565167531120767, -0.10058177053837325, -0.011426365891527044, -0.027460243011809632, -0.036857387111609005, -0.046195799060784595, -0.09183068009543512, -0.03245247581482806]\nRandomized Pearson G2: [-0.197891215644392, -0.11538767953142574, -0.23489350233836423, -0.15175398942284535, -0.15909663068868046, -0.1306511227926893, -0.17175835066677408, -0.1566105395514292, -0.18083547412138914, -0.2259320110296678]",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "netana_ex03.html#finding-a-graph-with-certain-properties",
    "href": "netana_ex03.html#finding-a-graph-with-certain-properties",
    "title": "Exercise 3",
    "section": "3) Finding a graph with certain properties",
    "text": "3) Finding a graph with certain properties\n(2 points) We discussed in the lecture that there are differing definitions for average (\\(C_G\\)) and global clustering coefficient (\\(C_{GG}\\)).\n\n\nGiven the double star graph family from Assignment 2.2, where we showed that both values diverge (\\(C_G \\rightarrow 1\\), \\(C_{GG} \\rightarrow 0\\)) for increasing \\(k\\) (periphery nodes): Can you find a graph family for which they approach the opposite values or at least go in the opposite direction compared to double star? In case, describe it and give the general formula for its clustering coefficients.\n\n\nWhat is the smallest graph you can find for which the two values differ?\n\n\n\nSolution\nTo provide a solution to the exercise: Provide the description, formula, and graph.\n\nIch hab keine ahnung wie ich das angehen sollte ausser das an Gpt zu senden.\n\nIn exercise 2.2 we looked at a network with two core nodes connected by an edge, and k peripheral nodes that each are connected exactly to the two core nodes (creating triangles): We observed the values for \\(C_G\\) and \\(C_{GG}\\) noting that for large \\(k\\), \\(C_G\\) was much larger than \\(C_{GG}\\).\nSo to flip it, we want: * Many connected triplets (to raise the denominator of \\(C_G\\)) * Few locally closed triangles for most nodes (to keep \\(C_G\\)) * But still many triangles in total (to raise numerator of \\(C_{GG}\\))\nWe define a graph family consisting of a clique of size \\(k\\) and \\(n−k\\) isolated nodes. The global clustering coefficient is always \\(C_{GG}=1\\), while the average local clustering coefficient is \\(C_G=\\frac{k}{n}\\), which goes to \\(0\\) as \\(n\\rightarrow\\infty\\)\nThis is the opposite of the double star graph family, where \\(C_G→1\\), but \\(C_{GG}→0\\).\n\nGlobal clustering \\(C_GG\\):\nThe clique has \\(\\begin{pmatrix}k\\\\3\\end{pmatrix}\\) triangles and \\(\\begin{pmatrix}k\\\\2\\end{pmatrix}\\cdot(k−2)\\) triplets.\nThe isolated nodes add nothing.\nSo: \\[C_{GG}=\\frac{3\\cdot\\begin{pmatrix}k\\\\3\\end{pmatrix}}{\\begin{pmatrix}k\\\\3\\end{pmatrix}\\cdot \\frac{k-2}{3}} = 1\\] (since it’s a clique → every triplet is closed → \\(C_{GG}=1\\))\nAverage local clustering \\(C_G\\):\nClique nodes have \\(C_v=1\\)\nIsolated nodes have \\(C_v=0\\)\nSo: \\[C_G=\\frac{k⋅1+(n−k)⋅0}{n}=\\frac{k}{n}\\]\n\\(→\\) goes to \\(0\\) as \\(n→∞\\) for fixed \\(k\\)\n\n\nsource\n\n\nclustered_core_with_isolates\n\n clustered_core_with_isolates (n=10, k=3)\n\nCreate a graph with a clique of size k and n-k isolated nodes.\n\n# Example graph with k=3 (clique) and n=10 (total nodes, 7 isolates)\nG = clustered_core_with_isolates(n=10, k=3)\nG1= clustered_core_with_isolates(n=15, k=6)",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "netana_ex03.html#giant-component",
    "href": "netana_ex03.html#giant-component",
    "title": "Exercise 3",
    "section": "4) Giant component",
    "text": "4) Giant component\n(2 points) In the last lecture we discussed the growth of the giant component during the creation of a graph by the random model. Run an experiment with increasing values of the edge probability and report the development of the giant component size (relative to the number of nodes in the graph) in relation to the probability values.\n\nSolution\nTo provide a solution to the exercise: Provide a diagram that shows the development of the size over the range of probability values.\nFor \\(n = 1000\\) nodes and \\(100\\) values in the probability range from \\(p\\in[0, 0.01]\\), at each step we generate a random graph \\(G(n,p)\\) and compute the relative size of the largest connected component \\[\\frac{\\text{size of largest component}}{n}\\]\nWe plot the critical threshold for the emergence of the giant component in as approximately \\(p = \\frac{1}{n}\\)\n\nBelow the threshold, most components are small.\nAround the threshold, the large connected component suddenly grows.\nAbove the threshold, the giant component rapidly dominates the graph.",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "netana_ex03.html#rich-club",
    "href": "netana_ex03.html#rich-club",
    "title": "Exercise 3",
    "section": "5) Rich Club",
    "text": "5) Rich Club\n(2 points) Select two networks from different categories of the SNAP Large Network Collection (https://snap.stanford.edu/data/index.html). Calculate the simple version of the rich club coefficient (\\(\\phi(k)\\) ) for each network and report it in a diagram as shown in the lecture. Then calculate the improved version \\(\\rho_{ran}(k)\\) and report the result.\n\nSolution\nTo provide a solution to the exercise: Shortly describe the networks, provide the diagrams and your verdict on rich club phenomenon.\nWhat is the formula for \\(\\phi (k)\\) and \\(\\rho_{ran} (k)\\).\nUse each formula to implement an algorithm solving this calculation and apply it to the graph. Alternately use a predefined algorithm if it is available and just\n\nNetworks Used\nWikipedia Vote Network (directed, but we make it undirected)\n\nNodes: 7,115\nEdges: 103,689\nDescription: Directed graph of Wikipedia users voting in admin elections. An edge from user A to B means A voted for B.\nSource: SNAP Wiki-Vote\n\nCalifornia road network (undirected)\n\nNodes: 1,965,206\nEdges: 2,766,607\nDescription: A road network of California. Intersections and endpoints are represented by nodes and the roads connecting these intersections or road endpoints are represented by undirected edges.\nSource: SNAP California Road\n\nEU Email Communication Network (sadly does not work because of self loops)\n\nNodes: 265,214\nEdges: 420,045\nDescription: Directed email exchange graph from a European research institution. An edge from A to B means A sent an email to B.\nSource: SNAP Email-EuAll\n\n\n\nRich Club\nThe rich club coefficient \\(φ(k)\\) measures the tendency of high-degree nodes (\\(degree &gt; k\\)) to be more densely interconnected than expected.\nSimple Rich-Club Coefficient: \\[ϕ(k)=\\frac{2E_k}{N_k(N_k−1)}\\]\nWhere:\n\n\\(N_k\\): number of nodes with degree &gt; kk\n\\(E_k\\): number of edges between those nodes\n\nNormalized Rich-Club Coefficient (Improved Version): \\[ρ(k)=\\frac{ϕ(k)}{ϕ_{rand}(k)}\\]\nWhere \\(ϕ_{rand}(k)\\) is the rich-club coefficient for a randomized version of the network with the same degree distribution.\nA value \\(ρ(k)&gt;1\\) indicates a rich club phenomenon.\n\n#Loading the graphs\n#G_email = nx.read_adjlist(\"../data/email-EuAll.txt\", nodetype=int, create_using=nx.DiGraph())\n#G_email = G_email.to_undirected()\nG_wiki = nx.read_adjlist(\"../data/Wiki-Vote.txt\", nodetype=int, create_using=nx.DiGraph())\nG_wiki = G_wiki.to_undirected()\n\n\nG_road = nx.read_adjlist(\"../data/roadNet-CA.txt\", nodetype=int)\n\n\nrich_road = nx.rich_club_coefficient(G_road, normalized=False)\n# doesnt work because it has self loops\n# rich_email = nx.rich_club_coefficient(G_email, normalized=False)\n\n\n# Simple rich-club coefficient\nrich_wiki = nx.rich_club_coefficient(G_wiki, normalized=False)\n\n\n# Normalized version\nrich_wiki_norm = nx.rich_club_coefficient(G_wiki, normalized=True, Q=10)\n\n\n# Normalized version\n#rich_road_norm = nx.rich_club_coefficient(G_norm, normalized=True, Q=10)\n# tooo slow, didnt finsih. I guess the Graph is just too large.",
    "crumbs": [
      "Exercise 3"
    ]
  },
  {
    "objectID": "abgabe_blatt_3_kalchschmid_wenzler.html",
    "href": "abgabe_blatt_3_kalchschmid_wenzler.html",
    "title": "Abgabe 03 Numerics",
    "section": "",
    "text": "#!/usr/bin/env python3\nimport numpy as np\nfrom scipy.linalg import solve_triangular, lu\n\nNumerische Mathematik - Abgabe Blatt Nr. 03\nTutor(-in): Samuel Inca Pilco\nFelix Kalchschmid, Leon Wenzler\n\nsource\n\ntrisolve\n\n trisolve (M:numpy.ndarray, b:&lt;built-infunctionarray&gt;,\n           threshold:float=1e-08)\n\n*Solves either a lower or upper triangular system of Mx = b. This is a minimized version of the trisolve function we submitted last sheet.\n:param A: triangular input matrix :param b: input vector of same size :return: solution vector x*\n\nsource\n\n\ngauss_decomposition_pivot\n\n gauss_decomposition_pivot (A:numpy.ndarray, threshold:float=1e-08)\n\n*Performs the full Gauss elimination algorithm using scaling and pivoting. Implements algorithm 2.5 from the lecture script, with optimizations where possible by replacing loops with vectorized operations.\n:param threshold: :param A: quadratic input matrix :return: scaling D, permutation P, lower triangular L, and upper triangular U matrices*\n\nsource\n\n\ngauss_elimination_pivot\n\n gauss_elimination_pivot (A:numpy.ndarray, b:numpy.ndarray,\n                          threshold:float=1e-08)\n\n*Uses the LU-decomposition with pivoting from gauss_decomposition_pivot to retrieve P, L, U. Then, solves the equation systems \\(Ly = Pb\\) and \\(Ux = y\\) to obtain the solutionv vector x.\n:param A: quadratic input matrix :param b: input vector of same size :return: solution vector x*\n\nsource\n\n\nsolve_linear_system\n\n solve_linear_system (M:numpy.ndarray, b:numpy.ndarray)\n\n*Solves a linear system Mx = b using NumPy’s built-in solver.\n:param M: Coefficient matrix :param b: Right-hand side vector :return: Solution vector x*\n\nsource\n\n\ncalculate_condition_numbers\n\n calculate_condition_numbers (M:numpy.ndarray)\n\n*Calculates and returns the Frobenius and 2-norm condition numbers of the matrix M.\n:param M: Input matrix :return: Frobenius and 2-norm condition numbers*\n\nsource\n\n\nlu_decomposition_solution\n\n lu_decomposition_solution (M:numpy.ndarray, b:numpy.ndarray)\n\n*Solves a linear system Mx = b using LU decomposition and forward/backward substitution.\n:param M: Coefficient matrix :param b: Right-hand side vector :return: Solution vector x*\n\nsource\n\n\ncompare_solutions\n\n compare_solutions (M:numpy.ndarray, b:numpy.ndarray)\n\n*Compares solutions obtained from direct solve and LU decomposition.\n:param M: Coefficient matrix :param b: Right-hand side vector*",
    "crumbs": [
      "Abgabe 03 Numerics"
    ]
  },
  {
    "objectID": "plotsine.html",
    "href": "plotsine.html",
    "title": "plotsine",
    "section": "",
    "text": "It’s simple to plot a graph in python. A great library for this task is matplotlib. We also need numpy for representing the numerical values that make up our graph.\nTest\nHere we define a sine function plotted using matplotlib\n\nsource\n\nplot_sine\n\n plot_sine (n)\n\nplot a sinus function\nWe set a number of samples that are generated and plot the function using the entries resulting from this.",
    "crumbs": [
      "plotsine"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "networks",
    "section": "",
    "text": "This project is a documentation for the topics of the Lecture Network Analysis at the University of Konstanz in the Summer Semester 2025. I cover most of the topics that were talked about in the lecture and provide explanations and examples in own words. My goal is also to enrich the information shown in the lecture with information I found myself in other sources while studying for this course.\nIn addition to the theoretical coverage I also provide my solutions for the exercises given in the course. By writing this documentation using jupyter notebooks and nbdev, I am able to combine both high quality markdown rendered text with results from actual python code and the full code itself that produces these results.",
    "crumbs": [
      "networks"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "networks",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall networks in Development mode\n# make sure networks package is installed in development mode\n$ pip install -e .\n# but prefer\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to networks\n$ nbdev_prepare",
    "crumbs": [
      "networks"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "networks",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/flupppi/networks.git\nor from conda\n$ conda install -c flupppi networks\nor from pypi\n$ pip install networks\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.\n\n\nTable of Contents\n\nLecture 1\n\nNetworks, Networks in the News\nApplications: Biological, Sociology, Human Interaction and Relations, Infrastructure, Criminalistics\nCriminalistics Example: Co-Offender Analysis\nTypical Network Analysis Tasks, Network Analysis Key Questions\nExample for Generating Networks\nFundamental Network Terminology, Adjacency Matrix, Directed-, Weighted-, Multivariate-Networks\nNetwork Categories: semantic=(Biological, Technological, Social, Information), structural=(Power-Law, Scale-free, Hierarchical, Small-World,…)\nInteraction in Netwoks: Multilayer, Multiplex\nNetwork Analysis Lifecycle\nNetwork Analysis Strength of Weak Ties\n\nLecture 2\n\nSimple measures on Graphs: Size, Density, Importance(Cut Vertex, Degree, Global/Local), Node Connectivity, Distance, Centrality\nNetwork Properties\n\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7",
    "crumbs": [
      "networks"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "networks",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "networks"
    ]
  }
]